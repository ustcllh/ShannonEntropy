<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>ShannonEntropy</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/custom.css">
		<link rel="stylesheet" href="css/theme/white.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown data-background-image="https://i.ytimg.com/vi/IoJ3KZ0HDJ0/maxresdefault.jpg" data-background-color="#222222">
					<script type="text/template">
						# Shannon Entropy
						Lihan Liu

					</script>
				</section>
				<section data-background-image="http://www.theblaze.com/wp-content/uploads/2015/06/matrix.jpg" >
					<div style="font-size:28px;background: rgba(236, 240, 241, .95)">
					<div data-markdown>
						## motivation

						### binary model systems

						An element of the system can be any site capable of two states, labeled as yes or no, red of blue, occupied or unoccupied.

						State of magnets
						$\uparrow \downarrow \downarrow \downarrow \uparrow \uparrow \downarrow \uparrow \downarrow ...$

						State of binary bit flow
						$01010000111000...$

					</div>
					<div style="color:red;font-size:32px">
						 Question: Can digital data be treated with statistical mechanics?
					</div>
				</div>
				</section>
				<section data-background-color="#222222">
					<div style="float:left;width:50%">
						<p data-markdown style="text-align:left;font-size:28px">
							Claude Elwood Shannon was an American mathematician, electrical engineer, and cryptographer known as "the father of information theory".

							Shannon Entropy was introduced in a landmark paper, *A mathematical Theory of Communication*, published in 1948.

							Shannon is noted for having founded information theory.
						</p>
						<p style="text-align:left;font-size:28px">
							Wikipedia:
							<a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a>.
							<br>
							Image:
							<a href="http://www.newyorker.com/tech/elements/claude-shannon-the-father-of-the-information-age-turns-1100100"> The New Yorker</a>.
						</p>
					</div>
					<div style="float:right;width:40%">
						<img src="http://www.newyorker.com/wp-content/uploads/2016/04/Roberts-Claude-Shannon-782.jpg">
					</div>
				</section>

				<section>
					<h6 >Entropy in Statistical Thermodynamics</h6>
					<div >
					<div style="float:left;width:49%;font-size:26px">
						<div data-markdown style="text-align:center">
							Boltzmann entropy formula

							$\sigma=log\ g$

							In Boltzmann's definition, entropy is a measure of the number of possible microstates of a system in **thermodynamic equilibrium**.
						</div>
					</div>
					<div style="float:right;width:49%;font-size:26px">
						<div data-markdown style="text-align:center" >
						Gibbs entropy formula

						$ s = - \sum_{i}^{} p_i \ log\ p_i $

						A general form, used to identify the entropy of a system. The microstates of such a thermodynamic system are **not equally probable**.
						</div>
					</div>
					<div style="clear:both;width:100%;font-size:26px">
						<div data-markdown style="text-align:center">
						In equilibrium, all microstates are equally likely, so we obtain with
						$ p_i = \frac{1}{g}$

						\begin{align}
						 s & = -  \sum_{1}^{g} \frac{1}{g} log\ \frac{1}{g} =-log\ \frac{1}{g} \\\\
						 & =\sigma
						\end{align}
						</div>
					</div>
</div>
				</section>
				<section>
					Shannon Entropy or Information Entropy
					<div data-markdown>
						$H=- \sum_{i}^{} p_i \ log_b \ p_i$

						If $b=2$, the unit of entropy is bit.
					</div>
				</section>


				<section data-background-image="http://cdn.wonderfulengineering.com/wp-content/uploads/2017/03/Coin-Flip-randomness-2.jpg">
					<div style="width:49%;height:60%;float:left;font-size:24px;background: rgba(236, 240, 241, .8)">
					<div data-markdown>
						- Experiment 1 with a fair coin
						- Number of possible events: N = 2
						- Probability of obtaining heads:

							$p_1=50\%$
						-	Probability of obtaining tails:

							$p_2=50\%$
						-	Shannon Entropy

							$H=-p_1 log_2 p_1-p_2 log_2 p_2=1(bit)$
					</div>
				</div>

				<div style="width:49%;height:80%;float:right;font-size:24px;background: rgba(236, 240, 241, .8)">
					<div data-markdown>
						- Experiment 2 with a double headed coin
						- Number of possible events: N = 1
						- Probability of obtaining heads:

							$p_1=100\%$
						-	Shannon Entropy

							$H=-p_1 log_2 p_1=0 (bits)$
					</div>
				</div>

				</section>

				<section data-background-color="#EEEEEE">
					<div style="width:49%;height:70%;float:left;font-size:32px">
						<div data-markdown style="text-align:left">
							After throwing a fair coin...

							We have obtained 1 bit of information.

							Or, uncertaintity has reduced by 1 bit.

							While, aftering throwing a double headed coin...

							We gained nothing.

						</div>
						<div style="text-align:left;color:red">
							Shannon Entropy quantifies information!
						</div>
					</div>

					<img  width="50%" src="http://ahmedalkiremli.com/wp-content/uploads/2014/09/Learn-Ahmed-Al-Kiremli.jpg" style="border:5" align="right">
				</section>

				<section>

					<div data-markdown style="float:left;width:60%;font-size:24px;text-align:left">
						Shannon Entropy can measure how much information is produced on the average for each letter of a text in the natural language.

						Findings:

						1.	For 27 characters (space included) with equal probability
							*	$H=4.75 \ (bits\ per\ character)$
							*	Maximum of information gained per character, theoretically

						2.	For 27 characters (space included) with different letter frequency
								*	$H=4.09 \ (bits\ per\ character)$
								*	neglect written conventions such as prefix, surfix (un-,in-,-ed,-tion)

						Data: [Letter Distribution](http://www.macfreek.nl/memory/Letter_Distribution)
					</div>
					<div style="float:right;width:40%">
					<img src="img/frequency_table.png">
					<img src="img/frequency_hist.png">
				</div>

				</section>

				<section>
					<div data-markdown style="float:left;width:60%;font-size:26px;text-align:left">
						In consideriation of probability unbalance and written convention

						*	$H=0.6 \sim 1.3 \ (bits\ per\ character)$
						*	Experimentally, based on the ability of human subjects to guess successive characters in text

						Imformation Redundancy: Language may not be efficient enough.
						To make it more effective:

						1.	Finding more efficient encoding method
						2.	Data compression
					</div>
					<div style="float:right;width:40%">
					<img style="border:0" src="http://telestreamblog.telestream.net/wp-content/uploads/2014/12/shutterstock_171118730.jpg">
				</div>

				</section>

				<section>


				<div style="float:right;width:40%">
				<img style="border:0" src="http://lqcc.ustc.edu.cn/zky/public/inc/editer/attached/image/20150407/20150407115107_51064.jpg">
			</div>
			<div style="float:left;width:60%">
				<h6> Maxwell's Demon</h6>
				<div data-markdown style="font-size:24px;text-align:left">
					Maxwell's demon is a thought experiment created by James Clerk Maxwell. A demon controls a small door between two chambers of gas.

					As individual gas molecules approach the door, the demon quickly opens and shuts the door so that fast molecules pass into the other chamber, while slow molecules remain in the first chamber.

					Because faster molecules are hotter, the demon's behavior causes one chamber to warm up as the other cools, thus decreasing entropy and violating the Second Law of Thermodynamics.

					Wikipedia: [Maxwell's demon](https://en.wikipedia.org/wiki/Maxwell%27s_demon)

				</div>
			</div>
</section>

<section >
	<div style="float:right;width:40%">
	<img style="border:0" src="https://real.social/wp-content/uploads/2015/01/Not-free.jpg">
</div>

<div style="float:left;width:60%">
	<h6>INFORMATION NOT FREE</h6>
	<div data-markdown style="font-size:24px;text-align:left">
		To deterine whether to let a molecule through, the demon must:

		1.	acquire information
		2.	discard or store
		3.	erase (thermodynamically irreversible)

		The measurement process comsumes energy, also increases entropy.

		Furthermore, **Landauer's principle** quantifies the entropy increase and release of heat due to manipulation of information.

		But that's another story...
	</div>
</div>

</section>
				<section >
					<div data-markdown style="width:60%;float:left;font-size:20px;text-align:left">
						### Reference
						[1].	A mathematical Theory of Communication, C.E. Shannon.

						[2].	Prediction and Entropy of Printed English, C.E. Shannon.

						[3].	Entropy(information theory), Wikipedia.

						[4].	Entropy (statistical thermodynamics), Wikipedia.

						[5].	Maxwell's demon, wikipedia.

						[6].	Landauer's principle, wikipedia.

						[7].	[Entropy and Redundancy in English](http://people.seas.harvard.edu/~jones/cscie129/papers/stanford_info_paper/entropy_of_english_9.htm).

						[8].	Information Theory, Claude Shannon, Entropy, Redundancy, Data Compression & Bits, [YouTube](https://www.youtube.com/watch?v=JnJq3Py0dyM&t=434s).

						[9].	[Letter Distribution](http://www.macfreek.nl/memory/Letter_Distribution).

						### Acknowledgement
						Online presentation based on [reveal.js](https://github.com/hakimel/reveal.js), an open-source html presentation framework.

						Follow me on [Github](https://github.com/ustcllh).


					</div>
					<div style="width:40%;float:right;top:90%">
					<img style="border:0" src="https://www.drupal.org/files/thanks.gif">
				</div>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: false,
				slideNumber: true,
				math: {
					mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
					config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
				},


				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/math/math.js', async: true }
				]
			});
		</script>
	</body>
</html>
